# 更多网络 I/O 模式

在上一节，通过学习阻塞式 I/O 及与线程或进程的配合，基本已经覆盖了 socket 编程的知识点，本节将更进一步，描述另外四种网络 I/O 的实现。

再复习一下，Unix 网络编程中，存在五种 I/O 模型，它们分别为：

- 阻塞式 I/O 模型
- 非阻塞式 I/O 模型
- I/O 复用模型
- 信号驱动式 I/O 模型
- 异步 I/O 模型

## 非阻塞式 I/O

非阻塞模式通过为文件描述符设置非阻塞标志位，从而达到在**数据准备阶段不阻塞**的效果。注意，这里强调数据准备阶段是因为，非阻塞式 I/O 在数据从内核缓冲区复制到用户空间时依然是阻塞的。非阻塞 I/O 模式仅为其设置标志位而不进行其他动作，所以使用者还需要在后续使用 write/read 等调用配合循环，手动对数据进行轮询的操作。这里首先给出一个非阻塞式 I/O 操作文件的例子。

[none_block_file](../src/network/advanced_io/none_block_file.c ':include')

可以看到写入普通文件时，无错误，一次完成

```bash
./a.out < /etc/services > tmp.file
```

而输出到终端时，会产生很多错误信息，可以看到错误代码为 11,错误消息为`Resource temporarily unavailable`。通过查阅头文件得知，11 号错误正是 EWOULDBLOCK，也即 EAGAIN，代表 Operation would block，Try again。这是因为终端程序一次可以接受的数据量一般不会达到如此之大的数据，所以会产生如此的错误。

```bash
./a.out </etc/services 2>stderr.out
```

---

看过非阻塞式的本地文件处理，再来看一下非阻塞式的 socket 服务端是如何实现的。其也可用上节中的循环客户端程序来测试。

[none_block_file](../src/network/advanced_io/none_block_file.c ':include')

可以看到，其可以以非阻塞的形式处理 accept 连接，读取客户端数据，以及向客户端发送数据。但是非阻塞 I/O 模式的手动实现需要轮询，这样会占用相当多的 cpu 资源。同时，在轮询间的间隔也不好掌握，所以手动实现非阻塞式 I/O 在生产中并不常见，对于较为简单的场景，阻塞式 I/O 配合多线程以及多进程更为实用。同时，如果服务端程序已经在等待一个客户端的输入，那么后续到来的客户端将被阻塞，因为此程序同时只能对一个文件描述符进行监视，这也是为什么 I/O 多路复用模型存在的重要原因。更为常用的模式是下面将描述的 I/O 复用模型。

## I/O 多路复用模型

I/O 多路复用模型最明显的好处是可以在同一个进程中监听多个文件描述符，也即一个进程同时处理多路 IO，这是前两种 I/O 模型无法实现的。I/O 多路复用模型最典型的实现分别为 select,poll 以及 epoll。本节将提供源代码分别进行描述。

### select

同样，可以使用前一节的循环客户端用来测试。可以看到程序中同时监控多个文件描述符。首先将主程序中的监听描述符加入 fd_set,随后监听开始。在主循环中，当有事件发生时，遍历 fd_set，根据不同的情况做出不同的动作。此时若为 accept 则不会阻塞，因为事件已经就续。但是如果是 read 事件，在数据准备阶段不会阻塞，在 I/O 阶段仍会阻塞。

[select](../src/network/advanced_io/select.c ':include')

### poll

### epoll

## 信号驱动式 I/O 模型

Ref: [[1]](https://www.bilibili.com/video/BV1pp4y1e7xN?p=6)
